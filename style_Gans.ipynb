{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from numpy import asarray, savez_compressed\n",
    "from PIL import Image\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from matplotlib import pyplot\n",
    "from math import sqrt\n",
    "from numpy import load, asarray, zeros, ones\n",
    "from numpy.random import randn, randint\n",
    "from skimage.transform import resize\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Conv2D, UpSampling2D, AveragePooling2D, LeakyReLU, Layer, Add\n",
    "from keras.constraints import max_norm\n",
    "from keras.initializers import RandomNormal\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename):\n",
    "    image = Image.open(filename)\n",
    "    image = image.convert('RGB')\n",
    "    pixels = asarray(image)\n",
    "    return pixels\n",
    " \n",
    "def extract_face(model, pixels, required_size=(128, 128)):\n",
    "    faces = model.detect_faces(pixels)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    x1, y1, width, height = faces[0]['box']\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face_pixels = pixels[y1:y2, x1:x2]\n",
    "    image = Image.fromarray(face_pixels)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array\n",
    " \n",
    "def load_faces(directory, n_faces):\n",
    "    model = MTCNN()\n",
    "    faces = list()\n",
    "    for filename in listdir(directory):\n",
    "        pixels = load_image(directory + filename)\n",
    "        face = extract_face(model, pixels)\n",
    "        if face is None:\n",
    "            continue\n",
    "        faces.append(face)\n",
    "        print(len(faces), face.shape)\n",
    "        if len(faces) >= n_faces:\n",
    "            break\n",
    "    return asarray(faces)\n",
    " \n",
    "directory = 'img_align_celeba/'\n",
    "all_faces = load_faces(directory, 50000)\n",
    "print('Loaded: ', all_faces.shape)\n",
    "savez_compressed('img_align_celeba_128.npz', all_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PixelNormalization, self).__init__(**kwargs)\n",
    " \n",
    "    def call(self, inputs):\n",
    "        values = inputs**2.0\n",
    "        mean_values = backend.mean(values, axis=-1, keepdims=True)\n",
    "        mean_values += 1.0e-8\n",
    "        l2 = backend.sqrt(mean_values)\n",
    "        normalized = inputs / l2\n",
    "        return normalized\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    " \n",
    "class MinibatchStdev(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MinibatchStdev, self).__init__(**kwargs)\n",
    " \n",
    "    def call(self, inputs):\n",
    "        mean = backend.mean(inputs, axis=0, keepdims=True)\n",
    "        squ_diffs = backend.square(inputs - mean)\n",
    "        mean_sq_diff = backend.mean(squ_diffs, axis=0, keepdims=True)\n",
    "        mean_sq_diff += 1e-8\n",
    "        stdev = backend.sqrt(mean_sq_diff)\n",
    "        mean_pix = backend.mean(stdev, keepdims=True)\n",
    "        shape = backend.shape(inputs)\n",
    "        output = backend.tile(mean_pix, (shape[0], shape[1], shape[2], 1))\n",
    "        combined = backend.concatenate([inputs, output], axis=-1)\n",
    "        return combined\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        input_shape = list(input_shape)\n",
    "        input_shape[-1] += 1\n",
    "        return tuple(input_shape)\n",
    " \n",
    "class WeightedSum(Add):\n",
    "    def __init__(self, alpha=0.0, **kwargs):\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "        self.alpha = backend.variable(alpha, name='ws_alpha')\n",
    " \n",
    "    def _merge_function(self, inputs):\n",
    "        assert (len(inputs) == 2)\n",
    "        output = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])\n",
    "        return output\n",
    " \n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return backend.mean(y_true * y_pred)\n",
    " \n",
    "def add_discriminator_block(old_model, n_input_layers=3):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    const = max_norm(1.0)\n",
    "    in_shape = list(old_model.input.shape)\n",
    "    input_shape = (in_shape[-2].value*2, in_shape[-2].value*2, in_shape[-1].value)\n",
    "    in_image = Input(shape=input_shape)\n",
    "    d = Conv2D(128, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(in_image)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = AveragePooling2D()(d)\n",
    "    block_new = d\n",
    "    for i in range(n_input_layers, len(old_model.layers)):\n",
    "        d = old_model.layers[i](d)\n",
    "    model1 = Model(in_image, d)\n",
    "    model1.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "    downsample = AveragePooling2D()(in_image)\n",
    "    block_old = old_model.layers[1](downsample)\n",
    "    block_old = old_model.layers[2](block_old)\n",
    "    d = WeightedSum()([block_old, block_new])\n",
    "    for i in range(n_input_layers, len(old_model.layers)):\n",
    "         d = old_model.layers[i](d)\n",
    "    model2 = Model(in_image, d)\n",
    "    model2.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "    return [model1, model2]\n",
    " \n",
    "def define_discriminator(n_blocks, input_shape=(4,4,3)):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    const = max_norm(1.0)\n",
    "    model_list = list()\n",
    "    in_image = Input(shape=input_shape)\n",
    "    d = Conv2D(128, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(in_image)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = MinibatchStdev()(d)\n",
    "    d = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(128, (4,4), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Flatten()(d)\n",
    "    out_class = Dense(1)(d)\n",
    "    model = Model(in_image, out_class)\n",
    "    model.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "    model_list.append([model, model])\n",
    "    for i in range(1, n_blocks):\n",
    "        old_model = model_list[i - 1][0]\n",
    "        models = add_discriminator_block(old_model)\n",
    "        model_list.append(models)\n",
    "    return model_list\n",
    " \n",
    "def add_generator_block(old_model):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    const = max_norm(1.0)\n",
    "    block_end = old_model.layers[-2].output\n",
    "    upsampling = UpSampling2D()(block_end)\n",
    "    g = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(upsampling)\n",
    "    g = PixelNormalization()(g)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    g = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "    g = PixelNormalization()(g)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    out_image = Conv2D(3, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "    model1 = Model(old_model.input, out_image)\n",
    "    out_old = old_model.layers[-1]\n",
    "    out_image2 = out_old(upsampling)\n",
    "    merged = WeightedSum()([out_image2, out_image])\n",
    "    model2 = Model(old_model.input, merged)\n",
    "    return [model1, model2]\n",
    " \n",
    "def define_generator(latent_dim, n_blocks, in_dim=4):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    const = max_norm(1.0)\n",
    "    model_list = list()\n",
    "    in_latent = Input(shape=(latent_dim,))\n",
    "    g  = Dense(128 * in_dim * in_dim, kernel_initializer=init, kernel_constraint=const)(in_latent)\n",
    "    g = Reshape((in_dim, in_dim, 128))(g)\n",
    "    g = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "    g = PixelNormalization()(g)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    g = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "    g = PixelNormalization()(g)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    out_image = Conv2D(3, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "    model = Model(in_latent, out_image)\n",
    "    model_list.append([model, model])\n",
    "    for i in range(1, n_blocks):\n",
    "        old_model = model_list[i - 1][0]\n",
    "        models = add_generator_block(old_model)\n",
    "        model_list.append(models)\n",
    "    return model_list\n",
    " \n",
    "def define_composite(discriminators, generators):\n",
    "    model_list = list()\n",
    "    for i in range(len(discriminators)):\n",
    "        g_models, d_models = generators[i], discriminators[i]\n",
    "        d_models[0].trainable = False\n",
    "        model1 = Sequential()\n",
    "        model1.add(g_models[0])\n",
    "        model1.add(d_models[0])\n",
    "        model1.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "        d_models[1].trainable = False\n",
    "        model2 = Sequential()\n",
    "        model2.add(g_models[1])\n",
    "        model2.add(d_models[1])\n",
    "        model2.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "        model_list.append([model1, model2])\n",
    "    return model_list\n",
    " \n",
    "def load_real_samples(filename):\n",
    "    data = load(filename)\n",
    "    X = data['arr_0']\n",
    "    X = X.astype('float32')\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X\n",
    " \n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    " \n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = generator.predict(x_input)\n",
    "    y = -ones((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "def update_fadein(models, step, n_steps):\n",
    "    alpha = step / float(n_steps - 1)\n",
    "    for model in models:\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, WeightedSum):\n",
    "                backend.set_value(layer.alpha, alpha)\n",
    " \n",
    "def train_epochs(g_model, d_model, gan_model, dataset, n_epochs, n_batch, fadein=False):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    half_batch = int(n_batch / 2)\n",
    "    for i in range(n_steps):\n",
    "        if fadein:\n",
    "            update_fadein([g_model, d_model, gan_model], i, n_steps)\n",
    "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        d_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "        d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "        z_input = generate_latent_points(latent_dim, n_batch)\n",
    "        y_real2 = ones((n_batch, 1))\n",
    "        g_loss = gan_model.train_on_batch(z_input, y_real2)\n",
    " \n",
    "def scale_dataset(images, new_shape):\n",
    "    images_list = list()\n",
    "    for image in images:\n",
    "        new_image = resize(image, new_shape, 0)\n",
    "        images_list.append(new_image)\n",
    "    return asarray(images_list)\n",
    " \n",
    "def summarize_performance(status, g_model, latent_dim, n_samples=25):\n",
    "    gen_shape = g_model.output_shape\n",
    "    name = '%03dx%03d-%s' % (gen_shape[1], gen_shape[2], status)\n",
    "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    square = int(sqrt(n_samples))\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(square, square, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X[i])\n",
    "    filename1 = 'plot_%s.png' % (name)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "    filename2 = 'model_%s.h5' % (name)\n",
    "    g_model.save(filename2)\n",
    "    print('>Saved: %s and %s' % (filename1, filename2))\n",
    " \n",
    "def train(g_models, d_models, gan_models, dataset, latent_dim, e_norm, e_fadein, n_batch):\n",
    "    g_normal, d_normal, gan_normal = g_models[0][0], d_models[0][0], gan_models[0][0]\n",
    "    gen_shape = g_normal.output_shape\n",
    "    scaled_data = scale_dataset(dataset, gen_shape[1:])\n",
    "    print('Scaled Data', scaled_data.shape)\n",
    "    train_epochs(g_normal, d_normal, gan_normal, scaled_data, e_norm[0], n_batch[0])\n",
    "    summarize_performance('tuned', g_normal, latent_dim)\n",
    "    for i in range(1, len(g_models)):\n",
    "        [g_normal, g_fadein] = g_models[i]\n",
    "        [d_normal, d_fadein] = d_models[i]\n",
    "        [gan_normal, gan_fadein] = gan_models[i]\n",
    "        gen_shape = g_normal.output_shape\n",
    "        scaled_data = scale_dataset(dataset, gen_shape[1:])\n",
    "        print('Scaled Data', scaled_data.shape)\n",
    "        train_epochs(g_fadein, d_fadein, gan_fadein, scaled_data, e_fadein[i], n_batch[i], True)\n",
    "        summarize_performance('faded', g_fadein, latent_dim)\n",
    "        train_epochs(g_normal, d_normal, gan_normal, scaled_data, e_norm[i], n_batch[i])\n",
    "        summarize_performance('tuned', g_normal, latent_dim)\n",
    " \n",
    "n_blocks = 6\n",
    "latent_dim = 100\n",
    "d_models = define_discriminator(n_blocks)\n",
    "g_models = define_generator(latent_dim, n_blocks)\n",
    "gan_models = define_composite(d_models, g_models)\n",
    "dataset = load_real_samples('img_align_celeba_128.npz')\n",
    "print('Loaded', dataset.shape)\n",
    "n_batch = [16, 16, 16, 8, 4, 4]\n",
    "n_epochs = [5, 8, 8, 10, 10, 10]\n",
    "train(g_models, d_models, gan_models, dataset, latent_dim, n_epochs, n_epochs, n_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
